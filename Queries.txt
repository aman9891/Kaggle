Q) How to find correlation matrix in Python ?
Q) How to find important features in Python ?
Q) Try understanding "An Extension to Imputation" - https://www.kaggle.com/dansbecker/handling-missing-values ?
Q) Read "Applying to Multiple Files" and "Conclusions" - https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding ?
Q) If error is least on our validation data for one model, can we be sure that same model will have least error on real data also ?

Q) Get the answer for commented question and then try all below cases ?
Q) Try using fit_transform() in pipeline with transformers and change point 19 and 21 ?
Q) Difference between fit_transform(), fit() and transform() in Pipeline ?
Q) Try using XGBRegressor() in Pipeline and try giving the fit() parameters studied ? Then try this with cross-validation approach ?
Q) What is the difference between loc() and iloc() ?
Q) What does this function before concatination do: final_numerical_training_data.reset_index(drop=True, inplace=True)
Q) How to use pipeline and cross_val_score to fit, predict and find error ?

Q) What is Faceting ?
Q) Difference between Feature Selection and Dimensionality Reduction ?
Q) How to specify maximum number of features in RFECV ?
Q) REFCV() calculates cross-validation error on training data, as that is what we are passing to it as parameter in fit() function? But shouldn't it be done on validation data?
Q) Read Pipeline() properly and implement end-to-end with it.

Q) Do we impute validation data with same values as learnt during on training data or according to validata data ?
Q) Is training on the complete data in one go same as training on split data seperately ?
Q) How to handle missing data in a column for most of the observations but when that column is important ?